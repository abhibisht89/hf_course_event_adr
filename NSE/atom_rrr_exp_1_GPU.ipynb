{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ee3598",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -U sentence-transformers rank_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1e2bf93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filesystem      Size  Used Avail Use% Mounted on\r\n",
      "devtmpfs         30G   72K   30G   1% /dev\r\n",
      "tmpfs            30G   12K   30G   1% /dev/shm\r\n",
      "/dev/xvda1      109G   94G   15G  87% /\r\n",
      "/dev/xvdf       4.9G  2.3G  2.4G  49% /home/ec2-user/SageMaker\r\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "079e3754",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder, util\n",
    "import gzip\n",
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cd987d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#We use the Bi-Encoder to encode all passages, so that we can use it with sematic search\n",
    "bi_encoder = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
    "bi_encoder.max_seq_length = 256     #Truncate long passages to 256 tokens\n",
    "top_k = 100                         #Number of passages we want to retrieve with the bi-encoder\n",
    "\n",
    "#The bi-encoder will retrieve 100 documents. We use a cross-encoder, to re-rank the results list to improve the quality\n",
    "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba38ac80",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia_filepath = 'simplewiki-2020-11-01.jsonl.gz'\n",
    "\n",
    "if not os.path.exists(wikipedia_filepath):\n",
    "    util.http_get('http://sbert.net/datasets/simplewiki-2020-11-01.jsonl.gz', wikipedia_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc774396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passages: 169597\n"
     ]
    }
   ],
   "source": [
    "passages = []\n",
    "with gzip.open(wikipedia_filepath, 'rt', encoding='utf8') as fIn:\n",
    "    for line in fIn:\n",
    "        data = json.loads(line.strip())\n",
    "\n",
    "        #Add all paragraphs\n",
    "        #passages.extend(data['paragraphs'])\n",
    "\n",
    "        #Only add the first paragraph\n",
    "        passages.append(data['paragraphs'][0])\n",
    "\n",
    "print(\"Passages:\", len(passages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64e04c21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88aceb44d2bc439099a2c6c86c0d6605",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We encode all passages into our vector space. This takes about 5 minutes (depends on your GPU speed)\n",
    "corpus_embeddings = bi_encoder.encode(passages, convert_to_tensor=True, show_progress_bar=True).to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98649844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open('corpus_embeddings_cpu.pkl', 'wb') as f:\n",
    "#     pickle.dump(corpus_embeddings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e781f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('corpus.pkl', 'wb') as f:\n",
    "#     pickle.dump(passages, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381e6ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We also compare the results to lexical search (keyword search). Here, we use \n",
    "# # the BM25 algorithm which is implemented in the rank_bm25 package.\n",
    "\n",
    "# from rank_bm25 import BM25Okapi\n",
    "# from sklearn.feature_extraction import _stop_words\n",
    "# import string\n",
    "# from tqdm.autonotebook import tqdm\n",
    "# import numpy as np\n",
    "\n",
    "\n",
    "# # We lower case our text and remove stop-words from indexing\n",
    "# def bm25_tokenizer(text):\n",
    "#     tokenized_doc = []\n",
    "#     for token in text.lower().split():\n",
    "#         token = token.strip(string.punctuation)\n",
    "\n",
    "#         if len(token) > 0 and token not in _stop_words.ENGLISH_STOP_WORDS:\n",
    "#             tokenized_doc.append(token)\n",
    "#     return tokenized_doc\n",
    "\n",
    "\n",
    "# tokenized_corpus = []\n",
    "# for passage in tqdm(passages):\n",
    "#     tokenized_corpus.append(bm25_tokenizer(passage))\n",
    "\n",
    "# bm25 = BM25Okapi(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a4ae434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will search all wikipedia articles for passages that\n",
    "# answer the query\n",
    "def search(query):\n",
    "    print(\"Input question:\", query)\n",
    "\n",
    "#     ##### BM25 search (lexical search) #####\n",
    "#     bm25_scores = bm25.get_scores(bm25_tokenizer(query))\n",
    "#     top_n = np.argpartition(bm25_scores, -5)[-5:]\n",
    "#     bm25_hits = [{'corpus_id': idx, 'score': bm25_scores[idx]} for idx in top_n]\n",
    "#     bm25_hits = sorted(bm25_hits, key=lambda x: x['score'], reverse=True)\n",
    "    \n",
    "#     print(\"Top-3 lexical search (BM25) hits\")\n",
    "#     for hit in bm25_hits[0:3]:\n",
    "#         print(\"\\t{:.3f}\\t{}\".format(hit['score'], passages[hit['corpus_id']].replace(\"\\n\", \" \")))\n",
    "\n",
    "    ##### Sematic Search #####\n",
    "    # Encode the query using the bi-encoder and find potentially relevant passages\n",
    "    question_embedding = bi_encoder.encode(query, convert_to_tensor=True)\n",
    "    hits = util.semantic_search(question_embedding, corpus_embeddings, top_k=top_k)\n",
    "    hits = hits[0]  # Get the hits for the first query\n",
    "\n",
    "    ##### Re-Ranking #####\n",
    "    # Now, score all retrieved passages with the cross_encoder\n",
    "    cross_inp = [[query, passages[hit['corpus_id']]] for hit in hits]\n",
    "    cross_scores = cross_encoder.predict(cross_inp)\n",
    "\n",
    "    # Sort results by the cross-encoder scores\n",
    "    for idx in range(len(cross_scores)):\n",
    "        hits[idx]['cross-score'] = cross_scores[idx]\n",
    "\n",
    "#     # Output of top-5 hits from bi-encoder\n",
    "#     print(\"\\n-------------------------\\n\")\n",
    "#     print(\"Top-3 Bi-Encoder Retrieval hits\")\n",
    "#     hits = sorted(hits, key=lambda x: x['score'], reverse=True)\n",
    "#     for hit in hits[0:3]:\n",
    "#         print(\"\\t{:.3f}\\t{}\".format(hit['score'], passages[hit['corpus_id']].replace(\"\\n\", \" \")))\n",
    "\n",
    "    # Output of top-5 hits from re-ranker\n",
    "    print(\"\\n-------------------------\\n\")\n",
    "    print(\"Top-5 Cross-Encoder Re-ranker hits\")\n",
    "    hits = sorted(hits, key=lambda x: x['cross-score'], reverse=True)\n",
    "    for hit in hits[0:5]:\n",
    "        print(\"\\t{:.3f}\\t{}\".format(hit['cross-score'], passages[hit['corpus_id']].replace(\"\\n\", \" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cebdc1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install -U sentence-transformers rank_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8687ed3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# from sentence_transformers import SentenceTransformer, CrossEncoder, util\n",
    "# import gzip\n",
    "# import os\n",
    "# import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6142c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #We use the Bi-Encoder to encode all passages, so that we can use it with sematic search\n",
    "# bi_encoder = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
    "# bi_encoder.max_seq_length = 256     #Truncate long passages to 256 tokens\n",
    "# top_k = 100                         #Number of passages we want to retrieve with the bi-encoder\n",
    "\n",
    "# #The bi-encoder will retrieve 100 documents. We use a cross-encoder, to re-rank the results list to improve the quality\n",
    "# cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad891f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wikipedia_filepath = 'simplewiki-2020-11-01.jsonl.gz'\n",
    "\n",
    "# if not os.path.exists(wikipedia_filepath):\n",
    "#     util.http_get('http://sbert.net/datasets/simplewiki-2020-11-01.jsonl.gz', wikipedia_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a513253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# passages = []\n",
    "# with gzip.open(wikipedia_filepath, 'rt', encoding='utf8') as fIn:\n",
    "#     for line in fIn:\n",
    "#         data = json.loads(line.strip())\n",
    "\n",
    "#         #Add all paragraphs\n",
    "#         #passages.extend(data['paragraphs'])\n",
    "\n",
    "#         #Only add the first paragraph\n",
    "#         passages.append(data['paragraphs'][0])\n",
    "\n",
    "# print(\"Passages:\", len(passages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db78ab3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We encode all passages into our vector space. This takes about 5 minutes (depends on your GPU speed)\n",
    "# corpus_embeddings = bi_encoder.encode(passages, convert_to_tensor=True, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a1d67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open('corpus_embeddings.pkl', 'wb') as f:\n",
    "#     pickle.dump(corpus_embeddings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde51b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('corpus.pkl', 'wb') as f:\n",
    "#     pickle.dump(passages, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e5274c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We also compare the results to lexical search (keyword search). Here, we use \n",
    "# # the BM25 algorithm which is implemented in the rank_bm25 package.\n",
    "\n",
    "# from rank_bm25 import BM25Okapi\n",
    "# from sklearn.feature_extraction import _stop_words\n",
    "# import string\n",
    "# from tqdm.autonotebook import tqdm\n",
    "# import numpy as np\n",
    "\n",
    "\n",
    "# # We lower case our text and remove stop-words from indexing\n",
    "# def bm25_tokenizer(text):\n",
    "#     tokenized_doc = []\n",
    "#     for token in text.lower().split():\n",
    "#         token = token.strip(string.punctuation)\n",
    "\n",
    "#         if len(token) > 0 and token not in _stop_words.ENGLISH_STOP_WORDS:\n",
    "#             tokenized_doc.append(token)\n",
    "#     return tokenized_doc\n",
    "\n",
    "\n",
    "# tokenized_corpus = []\n",
    "# for passage in tqdm(passages):\n",
    "#     tokenized_corpus.append(bm25_tokenizer(passage))\n",
    "\n",
    "# bm25 = BM25Okapi(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59662b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This function will search all wikipedia articles for passages that\n",
    "# # answer the query\n",
    "# def search(query):\n",
    "#     print(\"Input question:\", query)\n",
    "\n",
    "# #     ##### BM25 search (lexical search) #####\n",
    "# #     bm25_scores = bm25.get_scores(bm25_tokenizer(query))\n",
    "# #     top_n = np.argpartition(bm25_scores, -5)[-5:]\n",
    "# #     bm25_hits = [{'corpus_id': idx, 'score': bm25_scores[idx]} for idx in top_n]\n",
    "# #     bm25_hits = sorted(bm25_hits, key=lambda x: x['score'], reverse=True)\n",
    "    \n",
    "# #     print(\"Top-3 lexical search (BM25) hits\")\n",
    "# #     for hit in bm25_hits[0:3]:\n",
    "# #         print(\"\\t{:.3f}\\t{}\".format(hit['score'], passages[hit['corpus_id']].replace(\"\\n\", \" \")))\n",
    "\n",
    "#     ##### Sematic Search #####\n",
    "#     # Encode the query using the bi-encoder and find potentially relevant passages\n",
    "#     question_embedding = bi_encoder.encode(query, convert_to_tensor=True)\n",
    "#     hits = util.semantic_search(question_embedding, corpus_embeddings, top_k=top_k)\n",
    "#     hits = hits[0]  # Get the hits for the first query\n",
    "\n",
    "#     ##### Re-Ranking #####\n",
    "#     # Now, score all retrieved passages with the cross_encoder\n",
    "#     cross_inp = [[query, passages[hit['corpus_id']]] for hit in hits]\n",
    "#     cross_scores = cross_encoder.predict(cross_inp)\n",
    "\n",
    "#     # Sort results by the cross-encoder scores\n",
    "#     for idx in range(len(cross_scores)):\n",
    "#         hits[idx]['cross-score'] = cross_scores[idx]\n",
    "\n",
    "# #     # Output of top-5 hits from bi-encoder\n",
    "# #     print(\"\\n-------------------------\\n\")\n",
    "# #     print(\"Top-3 Bi-Encoder Retrieval hits\")\n",
    "# #     hits = sorted(hits, key=lambda x: x['score'], reverse=True)\n",
    "# #     for hit in hits[0:3]:\n",
    "# #         print(\"\\t{:.3f}\\t{}\".format(hit['score'], passages[hit['corpus_id']].replace(\"\\n\", \" \")))\n",
    "\n",
    "#     # Output of top-5 hits from re-ranker\n",
    "#     print(\"\\n-------------------------\\n\")\n",
    "#     print(\"Top-5 Cross-Encoder Re-ranker hits\")\n",
    "#     hits = sorted(hits, key=lambda x: x['cross-score'], reverse=True)\n",
    "#     for hit in hits[0:5]:\n",
    "#         print(\"\\t{:.3f}\\t{}\".format(hit['cross-score'], passages[hit['corpus_id']].replace(\"\\n\", \" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c798eccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input question: where is france\n",
      "\n",
      "-------------------------\n",
      "\n",
      "Top-5 Cross-Encoder Re-ranker hits\n",
      "\t9.442\tFrance ( or ; ), officially the French Republic (, ), is a country whose metropolitan territory is located in Western Europe and that also comprises various overseas islands and territories located in other continents. Metropolitan France extends from the Mediterranean Sea to the English Channel and the North Sea, and from the Rhine to the Atlantic Ocean. It is often referred to as \"L’Hexagone\" (\"The Hexagon\") because of the shape of its territory. France is a unitary semi-presidential republic with its main ideals expressed in the Declaration of the Rights of Man and of the Citizen.\n",
      "\t5.740\tThe capital of France is Paris. In the course of history, the national capital has been in many locations other than Paris.\n",
      "\t5.466\tMetropolitan France ( or \"la Métropole\") is the part of France that is in Europe. It can also be described as mainland France plus the island of Corsica. By contrast, Overseas France (\"France d'outre-mer\") is the collective name for all of the French overseas departments, territories and collectivities and New Caledonia. Metropolitan France and Overseas France together form the French Republic.\n",
      "\t5.334\tOverseas France () is the part of France that is outside of the European continent. It consists of all overseas departments, territories and collectivities. These territories have several different legal statuses and levels of autonomy. Overseas France includes island territories in the Atlantic, Pacific and Indian oceans, French Guiana in South America, and Adélie Land in Antarctica. Each inhabited territory is represented in both the French National Assembly and the French Senate (which together make up the Parliament of France).\n",
      "\t4.992\tÎle-de-France is a region of France. The capital city is Paris. It is also the capital city of France. In 2013 about 12 million people lived in the region. About 2.1 million people live in the city of Paris.\n"
     ]
    }
   ],
   "source": [
    "search(\"where is france\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f9f587",
   "metadata": {},
   "outputs": [],
   "source": [
    "search(\"where is france\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d19797",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b651189e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "iface = gr.Interface(fn=search, inputs=[\"text\"], outputs=\"textbox\").launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea81eb99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
